# Rectified Flow Training Configuration
# General settings
name: RectifiedFlow_x4_DF2K_200k
model_type: RectifiedFlowModel
scale: 4
num_gpu: 1  # set num_gpu: 0 for cpu mode
manual_seed: 10

# Dataset and data loader settings
datasets:
  train:
    name: DF2K
    type: PairedImageDataset
#    dataroot_gt: /8T1/dataset/SRDataset/ImageSR/DIV2K/DIV2K_train_HR
#    dataroot_lq: /8T1/dataset/SRDataset/ImageSR/DIV2K/DIV2K_train_LR_bicubic/X4
    dataroot_gt: /8T1/dataset/SRDataset/ImageSR/DF2K/train_HR
    dataroot_lq: /8T1/dataset/SRDataset/ImageSR/DF2K/train_LR_bicubic/X4
    filename_tmpl: '{}'
    io_backend:
      type: disk

    gt_size: 128  # Match DiT input_size
    use_hflip: true
    use_rot: true

    # data loader
    num_worker_per_gpu: 6
    batch_size_per_gpu: 4  # Smaller batch size for flow models
    dataset_enlarge_ratio: 100
    prefetch_mode: ~

  val:
    name: Set5
    type: PairedImageDataset
    dataroot_gt: /8T1/dataset/SRDataset/ImageSR/Set5/GTmod12
    dataroot_lq: /8T1/dataset/SRDataset/ImageSR/Set5/LRbicx4
    io_backend:
      type: disk

# Network structures
network_g:
  type: DiT
  input_size: 128
  patch_size: 4
  in_channels: 3
  dim: 384
  depth: 12
  num_heads: 16
  mlp_ratio: 4.0
  num_register_tokens: 4
  num_classes: ~

#network_g:
#  type: FlowUNet
#  dim: 64
#  init_dim: 64
#  channels: 3
#  dim_mults: [1, 2, 4, 8]
#  mean_variance_net: false
#  learned_sinusoidal_cond: false
#  random_fourier_features: false
#  dropout: 0.0
#  attn_dim_head: 32
#  attn_heads: 4
#  num_residual_streams: 2

# Rectified Flow specific settings
rectified_flow:
  time_cond_kwarg: 'times'
  predict: 'flow'  # 'flow' or 'noise'
  mean_variance_net: false
  noise_schedule: 'cosmap'  # 'cosmap' or custom function
  odeint_kwargs:
    atol: 1e-5
    rtol: 1e-5
    method: 'midpoint'
  clip_during_sampling: false
  clip_flow_during_sampling: null
  clip_values: [-1.0, 1.0]
  clip_flow_values: [-3.0, 3.0]
  use_consistency: false
  consistency_decay: 0.9999
  consistency_velocity_match_alpha: 1e-5
  consistency_delta_time: 1e-3
  consistency_loss_weight: 1.0
  immiscible: false
  ema_update_after_step: 100
  train_noise_std: 0.0001

# Path
path:
  pretrain_network_g: ~
  strict_load_g: true
  resume_state: ~

# Training settings
train:
  ema_decay: 0.999  # Set to 0 if using consistency flow matching
  optim_g:
    type: Adam
    lr: !!float 2e-4
    weight_decay: 0
    betas: [0.9, 0.99]

  scheduler:
    type: MultiStepLR
    milestones: [200000]
    gamma: 0.5

  total_iter: 200000
  warmup_iter: -1  # no warm up

  # Losses
  flow_opt:
    type: MSELoss
    loss_weight: 1.0
    # Alternative loss options:
#     type: PseudoHuberLoss
#     data_dim: 3
#     loss_weight: 1.0
    #
    # type: PseudoHuberLossWithLPIPS
    # data_dim: 3
    # lpips_kwargs: {}
    # loss_weight: 1.0

# Validation settings
val:
  val_freq: !!float 5e3  # Validate every 1000 iterations
  save_img: true

  metrics:
    psnr:
      type: calculate_psnr
      crop_border: 4
      test_y_channel: false
    ssim:
      type: calculate_ssim
      crop_border: 4
      test_y_channel: false
    fid:
      type: calculate_fid
      crop_border: 4

# Logging settings
logger:
  print_freq: 100
  save_checkpoint_freq: !!float 5e3
  use_tb_logger: true
  wandb:
    project: ~
    resume_id: ~

# Dist training settings
dist_params:
  backend: nccl
  port: 29500


